\documentclass[french]{article}

\usepackage{amsmath} % for \text
\usepackage{amssymb} % for \mathbb
\usepackage{amsthm} % for proof environment
\usepackage{hyperref} % for \autoref
\usepackage{cleveref} % for \cref
\usepackage{tikz} % for tikzpicture
\usepackage{graphicx} % for \includegraphics
\usepackage{subcaption} % for subfigure
\usepackage{float} % for H in figure
\usepackage{geometry} % for \newgeometry
\usepackage{fullpage} % for fullpage
\usepackage{babel} % for french
\usepackage[T1]{fontenc} % for french

\geometry{left = 1.8cm, right = 1.8cm}

\begin{document}

\begin{center}
    \textbf{\LARGE{Ecole Normale Supérieure de Paris-Saclay}}\\
\end{center}

\vspace{1cm}

\begin{center}
    \textbf{Rapport TER}
\end{center}

\noindent\rule{\textwidth}{0.6mm}
\bigskip
\begin{center}
    \textbf{\LARGE{TER - Voiture autonomes avec apprentissage par renforcement et lidar}}
\end{center}
\bigskip
\noindent\rule{\textwidth}{0.6mm}

\begin{center}
    \today
\end{center}

\bigskip
\begin{center}
    \textsc{Miquel Hugo}
\end{center}
\begin{center}
    \textsc{Plus Basile}
\end{center}

\begin{figure}[b]
    \centering
    \begin{minipage}[h]{0.45\textwidth}
        \raggedright
        \includegraphics[width=0.6\textwidth]{Images/Logo-ENS-Paris-Saclay.png}
    \end{minipage}
    \begin{minipage}[h]{0.45\textwidth}
        \raggedleft
        \includegraphics[width=0.6\textwidth]{Images/Logo-Universite-Paris-Saclay.jpg}
    \end{minipage}
\end{figure}


\newpage

\tableofcontents

\newpage

\section{Introduction}

\subsection{Contexte}

Les voitures autonomes sont un sujet de recherche très actif depuis quelques années. 
En effet, elles pourraient révolutionner le monde des transports en permettant de réduire les accidents de la route, 
de diminuer la consommation d'énergie et de réduire les embouteillages. 
Cependant, il reste encore de nombreux défis à relever pour que les voitures autonomes soient utilisées à grande échelle.
En particulier, il est nécessaire de développer des algorithmes d'apprentissage par renforcement qui permettent à une 
voiture autonome d'apprendre à conduire de manière autonome.

\subsection{Objectif et travail réalisé}
L'objectif de ce TER est de développer un algorithme d'apprentissage par renforcement qui permet à une 
voiture RC au format $1/10^{\text{ème}}$ de conduire de manière autonome sur un circuit. Dans un premier temps, nous 
avons utilisé Webots pour la simulation, gym et stable baselines pour l'apprentissage par renforcement.
Dans un second temps, nous avons transféré le réseau de neurones du simulateur à la voiture réelle.
La voiture est équipée d'un lidar qui permet de mesurer la distance entre la voiture et les murs du circuit.

\vspace{0.5cm}
\noindent
Note: Presenter Webots, la voiture réelle, la voiture sur simulateur, le lidar, le circuit etc...


\subsection{L'apprentissage par renforcement}
L'apprentissage par renforcement est une méthode d'apprentissage automatique qui permet à un agent 
d'apprendre à prendre des décisions en interagissant avec un environnement. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Images/RL.png}
    \caption{Schéma de l'apprentissage par renforcement}
\end{figure}

L'agent prend des actions dans l'environnement et reçoit une récompense en fonction de l'action qu'il a prise. 
L'objectif de l'agent est de maximiser la somme des récompenses qu'il reçoit au cours des itérations.

\vspace{0.5cm}
Lors de chaque étape, l'agent reçoit une observation de l'environnement dans lequel il évolue. Sur la base 
de cette observation, l'agent prend une décision parmi un ensemble d'actions possible appelé espace des actions. 
Cet espace peut dépendre de l'état dans lequel se trouve l'agent.

\vspace{0.5cm}
Un exemple simple est celui d'un jeu d'échecs dans lequel l'observation correspond à la position de chacune des 
pièces sur l'échiquier et l'espace des actions est l'ensemble des déplacements possibles des pièces. 
Naturellement, on souhaite que l'agent réalise la meilleure action possible suivant l'observation reçue. 
Pour atteindre ce but, l'agent applique une politique d'action (notée $\pi$) qu'il utilise pour sa prise de décision.
À chaque récompense obtenue, cette politique est mise à jour. On espère ainsi atteindre une politique optimale.

\vspace{0.5cm}
Pour entraîner un agent, plusieurs types de méthodes peuvent être utilisées. Ces méthodes estiment la somme des 
récompenses futures que l'agent devrait obtenir. Ces récompenses sont pondérées pour favoriser les récompenses 
à court terme. La politique obtenue est souvent modélisée par un réseau de neurones, dont l'actualisation modifie 
les poids du réseau.

\vspace{0.5cm}
\noindent
Les méthodes d'apprentissage par renforcement peuvent être classées en trois catégories principales :
\begin{itemize}
\item \textbf{Méthodes basées sur la valeur (value-based)} : Ces méthodes se concentrent sur l'estimation de la récompense cumulative optimale que l'agent peut obtenir. Elles cherchent à obtenir une récompense cumulative maximale.
\item \textbf{Méthodes basées sur la politique (policy-based)} : Ces méthodes se concentrent sur l'optimisation de la politique de l'agent. Les valeurs de récompense peuvent ne pas être calculées directement.
\item \textbf{Méthodes acteur-critique (actor-critic)} : Ces méthodes utilisent deux réseaux de neurones. Le premier réseau choisit l'action à effectuer, tandis que le second réseau évalue cette action en la comparant à l'action prévue.
\end{itemize}


\section{Simulation}

\subsection{Le simulateur Webots}
Le logiciel utilisé pour la simulation est Webots R2023b. Webots est un logiciel de simulation de robotique développé 
par Cyberbotics. Il permet de simuler des robots dans un environnement 3D que l'on peut personnaliser. Dans notre cas,
nous avons utilisé Webots pour simuler une voiture RC sur un circuit. Nous avons utilisé le langage de programmation
Python pour contrôler la voiture dans le simulateur.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Webots.png}
    \caption{Capture d'écran de Webots}
\end{figure}

\noindent
Note: Présenter le circuit, la voiture, le lidar, le code Python etc...

\subsection{Le circuit}
Le circuit dans l'environment de Webots a pour but de simuler un circuit réel sur lequel la voiture autonome doit
apprendre à conduire. Les murs du circuit sont composés de blocs de couleur différentes pour les bordures extérieur et 
intérieur du circuit. Ces murs ont une auteur d'une dizaine de centimètres qui permettent au lidar de mesurer la distance
entre la voiture et les murs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Circuit.png}
    \caption{Capture d'écran du circuit}
\end{figure}

\subsection{La voiture}
La voiture utilisée dans le simulateur est une voiture RC au format $1/10^{\text{ème}}$. Elle a pour objectif de
reproduire le plus fidèlement possible une voiture réelle. La voiture est équipée d'un lidar qui permet de mesurer la
distance entre la voiture et les murs du circuit.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Voiture.png}
    \caption{Capture d'écran de la voiture}
\end{figure}

\subsection{Le lidar}


\section{Simulation to real world}
L'objectif du SimToReal est de transférer un réseau de neurones entraîné sur un simulateur à une voiture réelle. 
Une fois le réseau de neurones entraîné, il est transféré à la voiture réelle pour qu'elle puisse conduire de manière 
autonome sur un circuit réel. Un des principaux défis du SimToReal est de reproduire le plus fidèlement possible les 
conditions du monde réel dans le simulateur.

\vspace{0.5cm}
La phase d'entraînement sur simulateur a plusieurs avantages. Tout d'abord, elle permet de réduire le temps et le coût
de l'entraînement. En effet, il est possible de simuler des milliers d'épisodes en quelques heures 
alors qu'il faudrait plusieurs jours pour réaliser le même nombre d'épisodes sur une voiture réelle. De plus,
la simulation permet de tester des scénarios dangereux pour la voiture réelle sans risquer de l'endommager.
Sur la voiture réelle, il faut aussi la replacer à la main après chaque crash dans un mur. Il est donc plus
efficace de réaliser l'entraînement sur simulateur.

\section{Introduction du bruit pour améliorer la simulation}
Pour que la simulation soit la plus proche possible de la réalité, il est crucial d'introduire des éléments de bruit. 
Ces bruits peuvent affecter les mesures et les actions de la voiture, et ainsi permettre au réseau de neurones de 
mieux généraliser lors du passage au monde réel.


\subsection{Bruit sur les mesures}
Dans un environnement réel, les capteurs ne fournissent pas des mesures parfaites. Les mesures du Lidar peuvent 
toujours être affectées par de petites interférences ou des variations mineures. Pour simuler ces conditions, 
on peut ajouter un léger bruit gaussien aux mesures du Lidar. Cela permet au réseau de neurones de s'adapter 
à des données moins parfaites, similaires à celles qu'il rencontrera dans le monde réel.


\subsection{Bruit sur les actions}
Les actions de la voiture, telles que l'angle de direction ou la vitesse, peuvent également être sujettes à des 
variations imprévues. Par exemple, un servomoteur peut ne pas toujours répondre de manière identique à une même 
commande en raison de l'usure ou des variations de tension. Pour prendre en compte ces imperfections, on peut 
ajouter du bruit aux actions commandées par le réseau de neurones. Cela aide à rendre l'agent plus robuste face 
aux variations qu'il pourrait rencontrer sur une voiture réelle.



\section{Application à la voiture autonome}
Pour appliquer l'apprentissage par renforcement à une voiture autonome, il est essentiel de définir correctement 
l'espace d'observation et l'espace d'action en tenant compte des contraintes du monde réel.

\subsection{Espace d'observation}
L'espace d'observation doit inclure toutes les informations pertinentes que la voiture peut obtenir de son environnement.
Dans notre cas, nous utilisons le Lidar pour mesurer les distances aux obstacles. Si un système de contrôle de 
la vitesse est en place, la vitesse actuelle de la voiture pourrait également faire partie de l'espace d'observation.

\subsection{Espace d'action}
L'espace d'action est limité aux commandes que l'agent peut envoyer à la voiture. Cela inclut l'incrémentation de 
l'angle de direction via le servomoteur et l'incrémentation de la vitesse via le moteur.


\section{Entraînement du réseau de neurones}
\subsection{Algorithme d'apprentissage}
\subsubsection*{Circuit d'entraînement}
La piste d'entraînement utilisée est conçue pour offrir une variété de situations afin que la voiture puisse 
apprendre à gérer différentes circonstances qu'elle pourrait rencontrer en course. Le circuit est composé de virages 
à gauche et à droite, de longues lignes droites, de virages en épingle et d'une section en diagonale. 
La piste retenue est la suivante:

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{Images/Piste Entrainement.png}
\caption{Piste d'entraînement pour l'apprentissage par renforcement}
\end{figure}

Sur ce circuit, nous avons ajouté trois autres voitures qui roulent à vitesse modérée pour simuler des situations 
de dépassement. Ces voitures suivent un algorithme de conduite simple qui consiste à rester à distance des murs.

\vspace{0.5cm}
Pour cet environnement, nous avons également intégré un robot "Superviseur" qui repositionne les voitures à des 
positions aléatoires lorsque nous souhaitons recommencer un épisode. Ce "Superviseur" choisit aléatoirement 
une position et une direction sur le circuit pour replacer les voitures, assurant ainsi une situation initiale 
nouvelle à chaque début d'épisode. La communication avec le "Superviseur" se fait via un système émetteur-récepteur 
intégré dans Webots.


\subsubsection*{Environment Gym}
La bibliothèque Gym est une bibliothèque implémentée en Python qui permet de gérer la partie apprentissage par 
renforcement d'un réseau de neurones. Dans le cas de notre projet, Gym ne propose pas d'environnement adapté. 
Il a donc été nécessaire de créer un tout nouvel environnement. Gym exige qu'un environnement contienne les fonctions 
suivantes:
\begin{itemize}
    \item \textbf{get\_observation()} : fonction renvoyant les observations de l'environnement
    \item \textbf{get\_reward()} : fonction donnant la récompense selon l'action effectuée par l'agent
    \item \textbf{reset()} : fonction donnant la démarche pour repartir au début d'un épisode
    \item \textbf{step()} : fonction faisant évoluer l'environnement
\end{itemize}

\vspace{0.5cm}
\noindent
Toutes ces fonctions sont rassemblées dans une classe que nous avons nommée \texttt{WebotsGymEnvironment}. Dans cette classe, nous avons rajouté quatre fonctions propres à la voiture :
\begin{itemize}
\item \textbf{get\_lidar\_mm()} : Fonction qui renvoie un tableau de Lidar dans le bon format avec des valeurs cohérentes en mm.
\item \textbf{set\_vitesse\_m\_s()} : Fonction qui prend en argument une vitesse en m/s et qui la convertit en km/h avant de l'envoyer à la voiture.
\item \textbf{set\_direction\_degre()} : Fonction qui prend un angle en degré et le convertit en radian avant de l'envoyer à la voiture.
\end{itemize}
\noindent
Nous allons détailler par la suite chacune des fonctions.

\subsubsection*{get\_observation()}
\noindent
Dans la fonction \texttt{get\_observation()}, on appelle la fonction \texttt{get\_lidar\_mm()} pour récupérer 
les observations du Lidar. Ce tableau sera le tableau donné en entrée du réseau pour les observations du Lidar 
à "l'instant présent". Pour ce qui est du tableau à "l'instant précédent", on stocke à chaque appel de la 
fonction le tableau d'observation dans une variable interne de la classe. Si la fonction est appelée depuis 
la fonction \texttt{reset()}, on donne le même tableau pour les deux parties de l'espace d'observation concernant 
le Lidar puisque la voiture a été repositionnée. Pour la vitesse et la direction, Webots nous permet de mesurer 
la vitesse et la direction de la voiture dans le simulateur. Ce sont ces données que l'on donne au réseau de neurones. 
De même dans le cas où l'observation est demandée par la fonction \texttt{reset()}, on indique que ces deux grandeurs 
sont nulles. Il est à noter que les valeurs données à l'espace d'observation sont normalisées.

\subsubsection*{get\_reward()}
\noindent
Dans la fonction \texttt{get\_reward()}, on donne la récompense associée à l'état dans lequel se trouve la voiture. 
On distingue deux états possibles pour la voiture. Le premier état est celui d'une situation de collision. 
On regarde la plus petite valeur du tableau de Lidar actuel et si celui-ci est inférieur à 120 mm, on considère 
qu'il y a collision. Dans le cas de la collision, on donne un malus de -400 moins la vitesse de la voiture. 
Le deuxième état regroupe toutes les situations autres que celle de collision. On donne comme récompense ici 
une valeur dépendant de la vitesse actuelle de la voiture ainsi que la distance minimale donnée par le tableau de Lidar. 
Les fonctions de récompenses seront détaillées plus tard. On indique aussi ici si l'on a terminé l'épisode via la 
variable \texttt{done}.

\subsubsection*{reset()}
\noindent
Dans la fonction \texttt{reset()}, on indique la démarche à suivre lorsque l'on veut retourner au début d'un épisode. 
Le reset se fait dans le cas d'un crash de la voiture ou si le nombre d'actions autorisées par épisode est atteint. 
Au moment du reset, on donne des consignes de vitesse et d'angle nuls et on envoie une indication de reset au robot 
"Superviseur". À la fin de la routine de réinitialisation, on renvoie une observation.

\subsubsection*{step()}
\noindent
Dans la fonction \texttt{step()}, on indique que l'on fait un pas dans le processus d'apprentissage. Dans cette 
fonction, on fait avancer la voiture avec les actions récupérées depuis le réseau de neurones. Ensuite, on récupère 
une observation depuis le Lidar et on fait faire un pas au processus d'apprentissage. On calcule la récompense 
avant de retourner toutes les informations obtenues.


\section{Conclusion}


\end{document}